# -*- coding: utf-8 -*-
"""Sentiment Analysis task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GRSc9iBSdTiCma396wM6gpec32U-_apR
"""

import pandas as pd
import numpy as np

df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSReNRzjhxq3oT03rISUM7-5UcNf_flKxO0AjFGcPbx3HapArOSyucjKEszeNDWUX7Qymiwm1Bqt73A/pub?gid=53846645&single=true&output=csv')

df.head()

X = df['review']

y = df['sentiment']

message = X.copy()

import spacy

nlp = spacy.load('en_core_web_sm')

corpus = []

import re

"""**Text Preprocessing**"""

for review in message:
  review = re.sub(r'<[^>]+>', ' ', review) # removing HTML tags
  review = re.sub('[^a-zA-Z]', ' ' , review) # Only take alphanumaric text
  review = review.lower() # always lower case
  review = nlp(review) # spacy object
  review = [token.lemma_ for token in review if not token.is_stop] #TOkenization lemmatization and removing stop word
  review = ' '.join(review)
  corpus.append(review) # add to courpus

voc_Size = 20000 # vocab size
max_len = 256 #maximum sequence length

"""**Text Vectorization**"""

from keras.layers import TextVectorization

vectorize_layer = TextVectorization(max_tokens=voc_Size, output_mode='int', output_sequence_length=max_len) #initate vectorizer with vocabulary size and sequence size
rep = vectorize_layer.adapt(corpus)
vectorized_corpus = vectorize_layer(corpus) # Text Vectorzation

vectorized_corpus

# Final 2-D array representaton
x_final = np.array(vectorized_corpus)
y_final = np.array(y)

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_final = label_encoder.fit_transform(y_final)

"""**Dataset Splitting**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
x_final, y_final, test_size=0.20, random_state=42)

import keras
from keras import ops
from keras import layers

"""**Initiate the Bi-LSTM Model**"""

from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import Bidirectional
from keras.layers import Embedding
from keras import Model
from keras import Input

from keras.regularizers import l2

inputs = keras.Input(shape=(None,), dtype="int64")  # Input layer for sequences of length 100
x = Embedding(voc_Size, 64)(inputs)
print("After embadding ", x.shape)


x = Bidirectional(LSTM(32, return_sequences=True))(x)
x = Bidirectional(LSTM(32, dropout=0.5, kernel_regularizer=l2(0.001)))(x) #lstm layer with dropout and L2 optimization

outputs = layers.Dense(1, activation="sigmoid")(x)
bilstmModel = Model(inputs=inputs, outputs=outputs)
bilstmModel.summary()

"""**Run Bilstm Model on Dataset**"""

bilstmModel.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

bilstmModel.fit(X_train, y_train, batch_size=128, epochs=50, validation_data=(X_test, y_test))

"""**Prediction of BILSTM**"""

bilstm_pred_probs = bilstmModel.predict(X_test)
bilstm_pred = (bilstm_pred_probs > 0.5)

"""**Confusion Matrix of Bilstm**"""

from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score

# Compute confusion matrix
bilstm_conf_matrix = confusion_matrix(y_test, bilstm_pred)

print("Bi-LSTM Confusion Matrix:")
print(bilstm_conf_matrix)

import matplotlib.pyplot as plt
import seaborn as sns

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(bilstm_conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

"""**Accuracy, Recall, F1-Score of BiLSTM**"""

bilstm_accuracy = accuracy_score(y_test, bilstm_pred)
bilstm_recall = recall_score(y_test, bilstm_pred)
bilstm_f1 = f1_score(y_test, bilstm_pred)

print("Bi-LSTM Accuracy:", bilstm_accuracy)
print("Bi-LSTM Recall:", bilstm_recall)
print("Bi-LSTM F1 Score:", bilstm_f1)

"""**Initiating Transformer block**"""

class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.5):
        super().__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential(
            [layers.Dense(ff_dim, activation="relu"), layers.Dense(embed_dim),]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)

    def call(self, inputs, training=False):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

"""**Token Position Embadding**"""

class TokenAndPositionEmbedding(layers.Layer):
    def __init__(self, maxlen, vocab_size, embed_dim):
        super().__init__()
        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)

    def call(self, x):
        maxlen = ops.shape(x)[-1]
        positions = ops.arange(start=0, stop=maxlen, step=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions

"""**Creating Transformer Model**"""

embed_dim =32  # Embedding size for each token
num_heads = 2  # Number of attention heads
ff_dim = 32 # Hidden layer size in feed forward network inside transformer

inputs = layers.Input(shape=(256,))
embedding_layer = TokenAndPositionEmbedding(1024, voc_Size, embed_dim)
x = embedding_layer(inputs)
transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)
x = transformer_block(x)
x = layers.GlobalAveragePooling1D()(x)
x = layers.Dropout(0.5)(x) #drop Out layer
x = layers.Dense(20, activation="relu", kernel_regularizer=l2(0.001))(x) # Dense Layer with L2 optimization
x = layers.Dropout(0.5)(x) #drop Out layer
outputs = layers.Dense(1, activation="sigmoid")(x)

transformerModel = keras.Model(inputs=inputs, outputs=outputs)

transformerModel.summary()

"""**Train Dataset on Transformer Model**"""

transformerModel.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
history = transformerModel.fit(
    X_train, y_train, batch_size=128, epochs=50, validation_data=(X_test, y_test)
)

"""**Transformer Model Prediction**"""

transformer_pred_probs = transformerModel.predict(X_test)
transformer_pred = (transformer_pred_probs > 0.5)

"""**Confusion Matric for Transformer**"""

# Compute confusion matrix
transformer_conf_matrix = confusion_matrix(y_test, transformer_pred)

print("transformer Confusion Matrix:")
print(transformer_conf_matrix)

import matplotlib.pyplot as plt
import seaborn as sns

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(bilstm_conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

"""**Accuracy, Recall, F1-score of Transformer**"""

transformer_accuracy = accuracy_score(y_test, transformer_pred)
transformer_recall = recall_score(y_test, transformer_pred)
transformer_f1 = f1_score(y_test, transformer_pred)

print("transformer Accuracy:", transformer_accuracy)
print("transformer Recall:", transformer_recall)
print("transformer F1 Score:", transformer_f1)

"""**Testing a instance**"""

X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(
    x_final, y_final, df.index, test_size=0.20, random_state=42
)

test_idx = idx_test[0]  # this is the integer index of the row in df

# Original review and label
original_text = df.loc[test_idx, 'review']
original_label = df.loc[test_idx, 'sentiment']
print("Original review:", original_text)
print("Original label:", original_label)

# Preprocessed text
processed_text = corpus[test_idx]
print("Processed text:", processed_text)
bilstm_prediction = bilstmModel.predict(X_test[0].reshape(1, -1))
transformer_prediction = transformerModel.predict(X_test[0].reshape(1, -1))

print("Bi-LSTM Prediction:", bilstm_prediction)
print("Transformer Prediction:", transformer_prediction)

"""**Save The Models**"""

bilstmModel.save('bilstmModel.h5')
transformerModel.save('transformerModel.h5')





















